version: '3.0'

services:

  brand-estimator:
    build:
      context: brand-estimator
    image: brand-estimator:latest
    container_name: brand-estimator
    hostname: brand-estimator
    ports:
      - "8888:8888"
    depends_on:
      redis-server:
        condition: service_healthy
      pulsar-server:
        condition: service_healthy
      fleet-server:
        condition: service_healthy
    environment:
      - REDIS_HOSTNAME=redis-server
      - REDIS_PORT=6379
      - PULSAR_SERVICE_URL=pulsar://pulsar-server:6650
      - OTEL_EXPORTER=otlp
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://fleet-server:8200
      - OTEL_RESOURCE_ATTRIBUTES=service.name=brand-estimator,service.version=1.0
    healthcheck:
      interval: 5s
      retries: 10
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8888/actuator/health

  analytics-layer:
    build:
      context: analytics-layer
    image: analytics-layer:latest
    container_name: analytics-layer
    hostname: analytics-layer
    depends_on:
      pulsar-server:
        condition: service_healthy
      fleet-server:
        condition: service_healthy
    environment:
      - PULSAR_SERVICE_URL=pulsar://pulsar-server:6650
      - OTEL_EXPORTER_OTLP_ENDPOINT=fleet-server:8200
      - OTEL_RESOURCE_ATTRIBUTES=service.name=analytics-layer,service.version=1.0

  redis-server:
    image: redis:latest
    container_name: redis-server
    hostname: redis-server
    ports:
      - 6379:6379
    logging:
      driver: 'json-file'
      options:
        max-size: '2m'
        max-file: '5'
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s      

  pulsar-server:
    command: bin/pulsar standalone
    image: apachepulsar/pulsar:latest
    container_name: pulsar-server
    hostname: pulsar-server
    ports:
    - 8080:8080
    - 6650:6650
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8080/admin/v2/clusters/standalone

  fleet-server:
    image: docker.elastic.co/beats/elastic-agent:8.0.0
    container_name: fleet-server
    hostname: fleet-server
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    entrypoint: "/bin/bash"
    command:
      - "-l"
      - "-c"
      - "export && elastic-agent container -d *"
    ports:
      - "8220:8220"
      - "8200:8200"
    environment:
      FLEET_ELASTICSEARCH_HOST: "http://elasticsearch:9200"
      FLEET_SERVER_ENABLE: "1"
      FLEET_SERVER_HOST: "0.0.0.0"
      FLEET_SERVER_POLICY_ID: "${FLEET_SERVER_POLICY_ID:-fleet-server-apm-policy}"
      FLEET_SERVER_PORT: "8220"
      KIBANA_FLEET_HOST: "http://kibana:5601"
      KIBANA_FLEET_SETUP: "1"
      FLEET_SERVER_INSECURE_HTTP: "1"
      FLEET_SERVER_ELASTICSEARCH_INSECURE: "1"
    volumes:
      - ./environment:/usr/share/elastic-agent/.bash_profile
    healthcheck:
      test: ["CMD-SHELL", "curl -s -k http://localhost:8220/api/status | grep -q 'HEALTHY'"]
      retries: 300
      interval: 1s

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.0.0
    container_name: elasticsearch
    hostname: elasticsearch
    environment:
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - "network.host="
      - "transport.host=127.0.0.1"
      - "http.host=0.0.0.0"
      - "cluster.routing.allocation.disk.threshold_enabled=false"
      - "discovery.type=single-node"
      - "xpack.security.authc.anonymous.roles=remote_monitoring_collector"
      - "xpack.security.authc.realms.file.file1.order=0"
      - "xpack.security.authc.realms.native.native1.order=1"
      - "xpack.security.enabled=true"
      - "xpack.license.self_generated.type=trial"
      - "xpack.security.authc.token.enabled=true"
      - "xpack.security.authc.api_key.enabled=true"
      - "logger.org.elasticsearch=${ES_LOG_LEVEL:-error}"
      - "action.destructive_requires_name=false"
    ports:
      - "9200:9200"
    volumes:
      - "./fleet-server/elasticsearch/roles.yml:/usr/share/elasticsearch/config/roles.yml"
      - "./fleet-server/elasticsearch/users:/usr/share/elasticsearch/config/users"
      - "./fleet-server/elasticsearch/users_roles:/usr/share/elasticsearch/config/users_roles"
    healthcheck:
      interval: 20s
      retries: 10
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=500ms"]

  service-token:
    image: python:3.9-slim
    container_name: service-token
    hostname: service-token
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTICSEARCH_HOST: http://elasticsearch:9200
      ELASTICSEARCH_USERNAME: "${KIBANA_ES_USER:-admin}"
      ELASTICSEARCH_PASSWORD: "${KIBANA_ES_PASS:-changeme}"
    command:
      - "/bin/bash"
      - "-c"
      - "pip install requests && python /tmp/init-fleet.py"
    volumes:
      - ./fleet-server/init-fleet.py:/tmp/init-fleet.py
      - ./:/out

  kibana:
    image: docker.elastic.co/kibana/kibana:8.0.0
    container_name: kibana
    hostname: kibana
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      ELASTICSEARCH_USERNAME: "${KIBANA_ES_USER:-kibana_system_user}"
      ELASTICSEARCH_PASSWORD: "${KIBANA_ES_PASS:-changeme}"
      STATUS_ALLOWANONYMOUS: 'true'
    ports:
      - "5601:5601"
    volumes:
      - ./fleet-server/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml
    healthcheck:
      interval: 10s
      retries: 20
      test: ["CMD-SHELL", "curl -s http://localhost:5601/api/status | grep -q 'All services are available'"]

volumes:
  otel-with-pulsar:
    driver: local

networks:
  default:
    name: otel-with-pulsar
